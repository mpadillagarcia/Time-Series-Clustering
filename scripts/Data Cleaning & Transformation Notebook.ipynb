{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1353eb1c",
   "metadata": {},
   "source": [
    "# Limpieza de Datos para Clustering de series de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd36f8f",
   "metadata": {},
   "source": [
    "### Manuel Padilla garcía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tslearn.clustering import KernelKMeans\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "from tslearn.metrics import cdist_dtw\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "\n",
    "folder = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233895cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo Parquet\n",
    "file = \"data/BTC-USDT.parquet\"\n",
    "\n",
    "# Leer todo el archivo Parquet\n",
    "data = pd.read_parquet(file)\n",
    "\n",
    "# Seleccionar las últimas 10 filas\n",
    "rows = data.tail(10)\n",
    "\n",
    "# Configurar pandas para que no use la notación científica\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Mostrar las últimas 10 filas\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a2103",
   "metadata": {},
   "source": [
    "### 1. ELIMINAR ARCHIVOS NO -USDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4e85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def removeNoUSDTFiles(folder):\n",
    "    # Obtener la lista de archivos .parquet en la carpeta\n",
    "    parquetFiles = glob.glob(os.path.join(folder, \"*.parquet\"))\n",
    "\n",
    "    for file in parquetFiles:\n",
    "        # Obtener el nombre del archivo sin la extensión\n",
    "        fileName = os.path.splitext(os.path.basename(file))[0]\n",
    "        # Verificar si el archivo no termina en \"-USDT\"\n",
    "        if not fileName.endswith(\"-USDT\"):\n",
    "            # Eliminar el archivo\n",
    "            os.remove(file)\n",
    "            print(f\"Se eliminó {file}\")\n",
    "\n",
    "removeNoUSDTFiles(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a641407",
   "metadata": {},
   "source": [
    "### 2. ELIMINAR ARCHIVOS DE CRIPTOS UP Y DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416bdcf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def removeUpDownFiles(folder, prefixes):\n",
    "    # Listar todos los archivos en la carpeta\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    for file in files:\n",
    "        # Verificar si el archivo tiene alguno de los prefijos especificados\n",
    "        for prefix in prefixes:\n",
    "            if prefix in file:\n",
    "                # Eliminar el archivo\n",
    "                filePath = os.path.join(folder, file)\n",
    "                os.remove(filePath)\n",
    "                print(f\"Se eliminó {file}\")\n",
    "\n",
    "\n",
    "# Prefijos de los archivos a eliminar\n",
    "removePrefixes = [\"UP-\", \"DOWN-\"]\n",
    "\n",
    "# Llamar a la función para eliminar los archivos\n",
    "removeUpDownFiles(folder, removePrefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68f0cf",
   "metadata": {},
   "source": [
    "### 3. ELIMINAR COLUMNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c4219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columnas a eliminar\n",
    "fields = ['high','low','close','volume','number_of_trades',\n",
    "            'taker_buy_base_asset_volume','taker_buy_quote_asset_volume']\n",
    "\n",
    "def removeFields(folder, fields):\n",
    "    # Listar todos los archivos Parquet en la carpeta\n",
    "    files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "    for file in files:\n",
    "        # Ruta completa\n",
    "        filePath = os.path.join(folder, file)\n",
    "\n",
    "        # Leer el archivo Parquet\n",
    "        df = pd.read_parquet(filePath)\n",
    "\n",
    "        # Eliminar las columnas especificadas\n",
    "        df = df.drop(columns=fields, errors='ignore')\n",
    "        \n",
    "        # Guardar el nuevo DataFrame sin las columnas en un nuevo archivo Parquet\n",
    "        df.to_parquet(filePath)\n",
    "\n",
    "\n",
    "# Procesar archivos Parquet\n",
    "removeFields(folder, fields)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c52de2",
   "metadata": {},
   "source": [
    "### 4. QUITAR ÍNDICES DE ARCHIVOS PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015654fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todos los archivos Parquet en la carpeta\n",
    "files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "for file in files:\n",
    "    # Ruta completa del archivo Parquet\n",
    "    filePath = os.path.join(folder, file)\n",
    "\n",
    "    # Leer el archivo Parquet\n",
    "    df = pd.read_parquet(filePath)\n",
    "\n",
    "    # Resetear el índice para convertir el campo de fecha en una columna\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Guardar el DataFrame actualizado en el archivo Parquet\n",
    "    df.to_parquet(filePath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35f04f",
   "metadata": {},
   "source": [
    "### 5. ELIMINAR FECHAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1befba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dateFilter(file):\n",
    "    # Leer el archivo Parquet\n",
    "    df = pd.read_parquet(file)\n",
    "\n",
    "    # Reemplaza 'open_time' con el nombre correcto de la columna que contiene la fecha y hora de apertura\n",
    "\n",
    "    # Convertir la columna 'open_time' a tipo datetime\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "\n",
    "    # Filtrar las filas por fecha\n",
    "    df_DateFiltered = df[(df['open_time'] >= '2020-01-01') & (df['open_time'] < '2023-01-01')]\n",
    "\n",
    "    return df_DateFiltered\n",
    "\n",
    "def removeDates(folder):\n",
    "    # Listar todos los archivos Parquet en la carpeta\n",
    "    files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "    for file in files:\n",
    "        # Ruta completa del archivo\n",
    "        filePath = os.path.join(folder, file)\n",
    "\n",
    "        # Filtrar por fecha y guardar en un nuevo archivo Parquet\n",
    "        df_DateFiltered = dateFilter(filePath)\n",
    "        df_DateFiltered.to_parquet(filePath)\n",
    "\n",
    "removeDates(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeda912",
   "metadata": {},
   "source": [
    "### 6. RENOMBRAR COLUMNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0689f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Listar todos los archivos Parquet en la carpeta\n",
    "files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "for file in files:\n",
    "    # Ruta completa del archivo Parquet\n",
    "    filePath = os.path.join(folder, file)\n",
    "\n",
    "    # Leer el archivo Parquet\n",
    "    df = pd.read_parquet(filePath)\n",
    "\n",
    "    # Renombrar las columnas\n",
    "    df = df.rename(columns={\"open_time\": \"date\", \"open\": \"price\", \"quote_asset_volume\": \"volume_usdt\"})\n",
    "\n",
    "    # Guardar el DataFrame actualizado en el archivo Parquet\n",
    "    df.to_parquet(filePath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d090f0",
   "metadata": {},
   "source": [
    "### 7. AGRUPAR POR DÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bb0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la lista de archivos Parquet en la carpeta\n",
    "files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "# Iterar sobre cada archivo Parquet\n",
    "for file in files:\n",
    "    # Ruta completa del archivo Parquet\n",
    "    filePath = os.path.join(folder, file)\n",
    "\n",
    "    # Leer el archivo Parquet\n",
    "    df = pd.read_parquet(filePath)\n",
    "\n",
    "    # Convertir la columna \"date\" al formato de fecha deseado (YYYY-MM-DD)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    # Agrupar por día y calcular la media de los valores \"price\" y la suma de los valores \"volume\"\n",
    "    df_GroupedDates = df.groupby('date').agg({'price': 'mean', 'volume_usdt': 'sum'}).reset_index()\n",
    "\n",
    "    # Guardar el DataFrame resultante en un nuevo archivo Parquet (sobreescribiendo el archivo original)\n",
    "    df_GroupedDates.to_parquet(filePath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f5afe",
   "metadata": {},
   "source": [
    "### 8. COMPROBAR LOS VALORES NULOS Y 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5843ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar variables para contar las filas con valores nulos o 0 en la columna \"price\"\n",
    "nullTotalCount = 0\n",
    "ceroTotalCount = 0\n",
    "\n",
    "# Obtener la lista de archivos Parquet en la carpeta\n",
    "files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "# Iterar sobre cada archivo Parquet\n",
    "for file in files:\n",
    "    # Ruta completa del archivo Parquet\n",
    "    ruta_parquet = os.path.join(folder, file)\n",
    "\n",
    "    # Leer el archivo Parquet\n",
    "    df = pd.read_parquet(ruta_parquet)\n",
    "\n",
    "    # Contar filas con valores nulos o 0 en la columna \"price\" de cada archivo\n",
    "    nullCount = df['price'].isnull().sum()\n",
    "    ceroCount = (df['price'] == 0).sum()\n",
    "\n",
    "    # Sumar al total\n",
    "    nullTotalCount += nullCount\n",
    "    ceroTotalCount += ceroCount\n",
    "\n",
    "# Mostrar el total de filas con valores nulos o 0 en la columna \"price\" en todos los archivos\n",
    "print(\"Total de filas con valores nulos en la columna 'price' en todos los archivos:\", nullTotalCount)\n",
    "print(\"Total de filas con valores 0 en la columna 'price' en todos los archivos:\", ceroTotalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bf21a",
   "metadata": {},
   "source": [
    "### 9. ELECCIÓN DE INTERVALO TEMPORAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d65372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechas a comprobar (reemplaza con las fechas que desees)\n",
    "startDate = datetime(2021, 9, 13)\n",
    "finishDate = datetime(2022, 9, 13)  \n",
    "\n",
    "def generateRanking(folder):\n",
    "    # Inicializar un diccionario para almacenar el número de archivos con cada intervalo de 365 días\n",
    "    intervals = {}\n",
    "    \n",
    "    # Iterar sobre los archivos en el directorio\n",
    "    for archivo in os.listdir(folder):\n",
    "        if archivo.endswith(\".parquet\"):\n",
    "            filePath = os.path.join(folder, archivo)\n",
    "            \n",
    "            # Leer el archivo Parquet\n",
    "            data = pd.read_parquet(filePath)\n",
    "            \n",
    "            # Convertir el campo 'date' a objetos de datetime\n",
    "            data['date'] = pd.to_datetime(data['date'])\n",
    "            \n",
    "            # Obtener la fecha mínima y máxima del archivo\n",
    "            minDate = data['date'].min()\n",
    "            maxDate = data['date'].max()\n",
    "            \n",
    "            # Verificar si ambas fechas son válidas\n",
    "            if pd.notnull(minDate) and pd.notnull(maxDate):\n",
    "                # Iterar sobre los intervalos de 365 días dentro del rango de fechas del archivo\n",
    "                for startDate in pd.date_range(start=minDate, end=maxDate - timedelta(days=365)):\n",
    "                    finishDate = startDate + timedelta(days=365)\n",
    "                    \n",
    "                    # Verificar si ambos extremos del intervalo están presentes en el archivo\n",
    "                    if (data['date'] >= startDate).any() and (data['date'] <= finishDate).any():\n",
    "                        # Incrementar el contador para este intervalo\n",
    "                        intervals[(startDate, finishDate)] = intervals.get((startDate, finishDate), 0) + 1\n",
    "    \n",
    "    # Ordenar los intervalos por cantidad de archivos Parquet en orden descendente\n",
    "    ranking = sorted(intervals.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranking\n",
    "\n",
    "# Generar ranking de intervalos de fechas (top 10)\n",
    "print(\"\\nRanking de intervalos de fechas (Top 10):\")\n",
    "ranking = generateRanking(folder)\n",
    "for i, ((inicio, fin), n) in enumerate(ranking[:20], 1):\n",
    "    print(f\"{i}. Intervalo: {inicio.strftime('%Y/%m/%d')} - {fin.strftime('%Y/%m/%d')}, Número de criptomonedas: {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34e075",
   "metadata": {},
   "source": [
    "### 10. ELIMINAR CRIPTOMONEDAS QUE NO CONTIENEN EL INTERVALO DE FECHAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b41d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre los archivos a eliminar y borrarlos\n",
    "for file in missing_files:\n",
    "    filePath = os.path.join(folder, file)\n",
    "    if os.path.exists(filePath):\n",
    "        os.remove(filePath)\n",
    "        print(f\"Archivo {file} eliminado correctamente.\")\n",
    "    else:\n",
    "        print(f\"El archivo {file} no existe en el directorio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542b5c5",
   "metadata": {},
   "source": [
    "### 11. AGREGAR COLUMNAS EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la lista de archivos .parquet en la carpeta\n",
    "files = [file for file in os.listdir(data_folder) if file.endswith('.parquet')]\n",
    "\n",
    "# Iterar sobre cada archivo .parquet\n",
    "for file in files:\n",
    "    # Ruta completa al archivo .parquet\n",
    "    filePath = os.path.join(folder, file)\n",
    "    \n",
    "    # Leer el archivo .parquet en un DataFrame de Pandas\n",
    "    df = pd.read_parquet(filePath)\n",
    "    \n",
    "    # Calcular la EMA50 de la columna 'price'\n",
    "    df['ema50'] = df['price'].ewm(span=50, adjust=False).mean()\n",
    "    df['ema100'] = df['price'].ewm(span=100, adjust=False).mean()\n",
    "    df['ema200'] = df['price'].ewm(span=200, adjust=False).mean()\n",
    "    \n",
    "    \n",
    "    # Escribir el DataFrame modificado de vuelta al archivo .parquet\n",
    "    df.to_parquet(filePath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb764a",
   "metadata": {},
   "source": [
    "### 12. NORMALIZAR PRECIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la lista de archivos .parquet en la carpeta\n",
    "files = [file for file in os.listdir(folder) if file.endswith('.parquet')]\n",
    "\n",
    "# Iterar sobre cada archivo .parquet\n",
    "for file in files:\n",
    "    # Ruta completa al archivo .parquet\n",
    "    filePath = os.path.join(folder, file)\n",
    "    \n",
    "    # Leer el archivo .parquet en un DataFrame de Pandas\n",
    "    df = pd.read_parquet(filePath)\n",
    "    \n",
    "    # Calcular el valor máximo de las columnas\n",
    "    maxPrice = df['price'].max()\n",
    "    maxEma50 = df['ema50'].max()\n",
    "    maxEma100 = df['ema100'].max()\n",
    "    maxEma200 = df['ema200'].max()\n",
    "    \n",
    "    # Agregar una nueva columna 'price_norm' con los valores normalizados\n",
    "    df['price'] = df['price'] / maxPrice\n",
    "    df['ema50'] = df['ema50'] / maxEma50\n",
    "    df['ema100'] = df['ema100'] / maxEma100\n",
    "    df['ema200'] = df['ema200'] / maxEma200\n",
    "    \n",
    "    # Escribir el DataFrame modificado de vuelta al archivo .parquet\n",
    "    df.to_parquet(filePath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b294d",
   "metadata": {},
   "source": [
    "### 13. ELIMINAR FECHAS FUERA DEL INTERVALO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateFilter(file):\n",
    "    # Leer el archivo Parquet\n",
    "    df = pd.read_parquet(file)\n",
    "\n",
    "    # Reemplaza 'open_time' con el nombre correcto de la columna que contiene la fecha y hora de apertura\n",
    "\n",
    "    # Convertir la columna 'open_time' a tipo datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Filtrar las filas por fecha\n",
    "    df_DateFiltered = df[(df['date'] >= '2021-09-13') & (df['date'] <= '2022-09-13')]\n",
    "\n",
    "    return df_DateFiltered\n",
    "\n",
    "def removeDates(folder):\n",
    "    # Listar todos los archivos Parquet en la carpeta\n",
    "    files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "    for file in files:\n",
    "        # Ruta completa del archivo\n",
    "        filePath = os.path.join(folder, file)\n",
    "\n",
    "        # Filtrar por fecha y guardar en un nuevo archivo Parquet\n",
    "        df_DateFiltered = dateFilter(filePath)\n",
    "        df_DateFiltered = df_DateFiltered.reset_index()\n",
    "        df_DateFiltered.to_parquet(filePath)\n",
    "        \n",
    "\n",
    "removeDates(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas a eliminar\n",
    "fields = ['index']\n",
    "\n",
    "def removeFields(folder, fields):\n",
    "    # Listar todos los archivos Parquet en la carpeta\n",
    "    files = [f for f in os.listdir(folder) if f.endswith('.parquet')]\n",
    "\n",
    "    for file in files:\n",
    "        # Ruta completa\n",
    "        filePath = os.path.join(folder, file)\n",
    "\n",
    "        # Leer el archivo Parquet\n",
    "        df = pd.read_parquet(filePath)\n",
    "\n",
    "        # Eliminar las columnas especificadas\n",
    "        df = df.drop(columns=fields, errors='ignore')\n",
    "        \n",
    "        # Guardar el nuevo DataFrame sin las columnas en un nuevo archivo Parquet\n",
    "        df.to_parquet(filePath)\n",
    "\n",
    "\n",
    "# Procesar archivos Parquet\n",
    "removeFields(folder, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b798127",
   "metadata": {},
   "source": [
    "### TRANSFORMACIÓN DE ARCHIVOS DE MÉTRICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_distance(cluster1, cluster2):\n",
    "    # Calcula la distancia entre dos clusters utilizando la distancia mínima entre pares de puntos\n",
    "    min_distance = float('inf')\n",
    "    for i in range(len(cluster1)):\n",
    "        for j in range(len(cluster2)):\n",
    "            distance = np.linalg.norm(cluster1[i] - cluster2[j])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "    return min_distance\n",
    "\n",
    "def intra_cluster_dispersion(cluster):\n",
    "    # Calcula la dispersión intra-cluster como la media de las distancias entre pares de puntos en el cluster\n",
    "    total_distance = 0\n",
    "    num_points = len(cluster)\n",
    "    for i in range(num_points):\n",
    "        for j in range(i+1, num_points):\n",
    "            total_distance += np.linalg.norm(cluster[i] - cluster[j])\n",
    "    if num_points <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return total_distance / (num_points * (num_points - 1) / 2)\n",
    "\n",
    "def dunn_index(clusters):\n",
    "    # Calcula el índice de Dunn para un conjunto de clusters\n",
    "    min_inter_cluster_distance = float('inf')\n",
    "    max_intra_cluster_dispersion = 0\n",
    "    num_clusters = len(clusters)\n",
    "    \n",
    "    # Calcula la distancia mínima entre clusters\n",
    "    for i in range(num_clusters):\n",
    "        for j in range(i+1, num_clusters):\n",
    "            distance = cluster_distance(clusters[i], clusters[j])\n",
    "            if distance < min_inter_cluster_distance:\n",
    "                min_inter_cluster_distance = distance\n",
    "    \n",
    "    # Calcula la máxima dispersión intra-cluster\n",
    "    for i in range(num_clusters):\n",
    "        dispersion = intra_cluster_dispersion(clusters[i])\n",
    "        if dispersion > max_intra_cluster_dispersion:\n",
    "            max_intra_cluster_dispersion = dispersion\n",
    "    \n",
    "    # El índice de Dunn es la relación entre la distancia mínima entre clusters y la máxima dispersión intra-cluster\n",
    "    if max_intra_cluster_dispersion == 0:\n",
    "        return float('inf')\n",
    "    else:\n",
    "        return min_inter_cluster_distance / max_intra_cluster_dispersion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf709d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equity_score(distribution, total_elements):\n",
    "    max_elements = max(distribution)\n",
    "    min_elements = min(distribution)\n",
    "    if max_elements == min_elements:\n",
    "        return 1.0\n",
    "    return min_elements / max_elements\n",
    "\n",
    "def generate_distributions(total_elements, num_groups):\n",
    "    return set(itertools.combinations_with_replacement(range(total_elements), num_groups))\n",
    "\n",
    "def calculate_scores(total_elements, group_numbers):\n",
    "    rows = []\n",
    "    for num_groups in group_numbers:\n",
    "        distributions = generate_distributions(total_elements, num_groups)\n",
    "        num_distributions = len(distributions)\n",
    "        for distribution in distributions:\n",
    "            if sum(distribution) == total_elements:\n",
    "                score = equity_score(np.bincount(distribution), total_elements)\n",
    "                std_dev = np.std(distribution)\n",
    "                rows.append({'groups': num_groups, 'distribution': distribution, 'score': score, 'std_dev': std_dev})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['distribution'] = df['distribution'].apply(lambda x: ';'.join(map(str, x)))\n",
    "    df = df.groupby('groups').apply(lambda x: x.sort_values(by='std_dev')).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso\n",
    "total_cryptos = 200\n",
    "group_numbers = [3, 4, 5]\n",
    "results = calculate_scores(total_cryptos, group_numbers)\n",
    "results.to_csv('equity_scores.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff791797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta donde se encuentran los archivos CSV\n",
    "carpeta_metrics = 'metrics'\n",
    "\n",
    "# Lista de archivos CSV en la carpeta\n",
    "archivos_csv = [archivo for archivo in os.listdir(carpeta_metrics) if archivo.endswith('.csv')]\n",
    "\n",
    "# Procesar cada archivo CSV\n",
    "for archivo_csv in archivos_csv:\n",
    "    # Leer el archivo CSV\n",
    "    ruta_archivo = os.path.join(carpeta_metrics, archivo_csv)\n",
    "    df = pd.read_csv(ruta_archivo)\n",
    "    \n",
    "    # Filtrar las filas que contienen 'media' en la columna 'iteration'\n",
    "    # y mantener la primera fila (asumida como encabezado)\n",
    "    filtro = df['iteration'].str.contains('Media', case=False, na=False)\n",
    "    df_filtrado = df[filtro]\n",
    "    \n",
    "    # Guardar el DataFrame filtrado de vuelta al archivo CSV\n",
    "    df_filtrado.to_csv(ruta_archivo, index=False)\n",
    "\n",
    "    print(f\"Archivo procesado: {ruta_archivo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0535ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta donde se crearán los archivos CSV\n",
    "carpeta_charts = 'metrics/charts'\n",
    "\n",
    "# Crear la carpeta si no existe\n",
    "os.makedirs(carpeta_charts, exist_ok=True)\n",
    "\n",
    "# Lista de nombres de archivos CSV a crear\n",
    "archivos_csv = [\n",
    "    \"ema50-silhouette.csv\", \"ema100-silhouette.csv\", \"ema200-silhouette.csv\", \"price-silhouette.csv\",\n",
    "    \"ema50-dunn.csv\", \"ema100-dunn.csv\", \"ema200-dunn.csv\", \"price-dunn.csv\",\n",
    "    \"ema50-davies.csv\", \"ema100-davies.csv\", \"ema200-davies.csv\", \"price-davies.csv\",\n",
    "    \"ema50-calinski.csv\", \"ema100-calinski.csv\", \"ema200-calinski.csv\", \"price-calinski.csv\",\n",
    "    \"sk-timeserieskmeans-silhouette.csv\", \"sk-timeserieskmeans-dunn.csv\", \"sk-timeserieskmeans-davies.csv\", \"sk-timeserieskmeans-calinski.csv\",\n",
    "    \"sk-timeserieskmedoids-silhouette.csv\", \"sk-timeserieskmedoids-dunn.csv\", \"sk-timeserieskmedoids-davies.csv\", \"sk-timeserieskmedoids-calinski.csv\",\n",
    "    \"ts-timeserieskmeans-silhouette.csv\", \"ts-timeserieskmeans-dunn.csv\", \"ts-timeserieskmeans-davies.csv\", \"ts-timeserieskmeans-calinski.csv\",\n",
    "    \"ts-kshape-silhouette.csv\", \"ts-kshape-dunn.csv\", \"ts-kshape-davies.csv\", \"ts-kshape-calinski.csv\",\n",
    "    \"ts-kernelkmeans-silhouette.csv\", \"ts-kernelkmeans-dunn.csv\", \"ts-kernelkmeans-davies.csv\", \"ts-kernelkmeans-calinski.csv\"\n",
    "]\n",
    "\n",
    "# Crear cada archivo CSV vacío en la carpeta especificada\n",
    "for archivo_csv in archivos_csv:\n",
    "    ruta_archivo = os.path.join(carpeta_charts, archivo_csv)\n",
    "    # Abrir el archivo en modo de escritura y cerrarlo inmediatamente\n",
    "    open(ruta_archivo, 'w').close()\n",
    "    print(f\"Archivo creado: {ruta_archivo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2253fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta de entrada (metrics) y la carpeta de salida (charts)\n",
    "carpeta_metrics = 'metrics'\n",
    "carpeta_charts = os.path.join(carpeta_metrics, 'charts')\n",
    "\n",
    "# Crear la carpeta charts si no existe\n",
    "os.makedirs(carpeta_charts, exist_ok=True)\n",
    "\n",
    "# Lista de archivos en la carpeta metrics\n",
    "archivos_metrics = [archivo for archivo in os.listdir(carpeta_metrics) if archivo.endswith('.csv')]\n",
    "\n",
    "# Lista de archivos en la carpeta charts\n",
    "archivos_charts = [archivo for archivo in os.listdir(carpeta_charts) if archivo.endswith('.csv')]\n",
    "\n",
    "# Diccionario de condiciones de coincidencia\n",
    "condiciones = {\n",
    "    \"sktime-TimeSeriesKMeans\": \"sk-timeserieskmeans\",\n",
    "    \"TimeSeriesKMedoids\": \"timeserieskmedoids\",\n",
    "    \"tslearn-TimeSeriesKMeans\": \"ts-timeserieskmeans\",\n",
    "    \"KShape\": \"kshape\",\n",
    "    \"KernelKMeans\": \"kernelkmeans\"\n",
    "}\n",
    "\n",
    "# Función para verificar si un archivo está vacío\n",
    "def archivo_vacio(ruta):\n",
    "    return os.path.exists(ruta) and os.stat(ruta).st_size == 0\n",
    "\n",
    "# Procesar cada archivo CSV en metrics\n",
    "for archivo_metric in archivos_metrics:\n",
    "    ruta_archivo_metric = os.path.join(carpeta_metrics, archivo_metric)\n",
    "    \n",
    "    # Leer el archivo CSV si no está vacío\n",
    "    if not archivo_vacio(ruta_archivo_metric):\n",
    "        df_metric = pd.read_csv(ruta_archivo_metric)\n",
    "        \n",
    "        # Determinar los archivos de charts correspondientes\n",
    "        for key, value in condiciones.items():\n",
    "            if key in archivo_metric:\n",
    "                for archivo_chart in archivos_charts:\n",
    "                    if value in archivo_chart:\n",
    "                        ruta_archivo_chart = os.path.join(carpeta_charts, archivo_chart)\n",
    "                        \n",
    "                        # Leer o inicializar el DataFrame del archivo chart si no está vacío\n",
    "                        if not archivo_vacio(ruta_archivo_chart):\n",
    "                            df_chart = pd.read_csv(ruta_archivo_chart)\n",
    "                        else:\n",
    "                            df_chart = pd.DataFrame()\n",
    "                        \n",
    "                        # Concatenar el DataFrame del archivo metric con el correspondiente DataFrame del archivo chart\n",
    "                        df_chart = pd.concat([df_chart, df_metric], ignore_index=True)\n",
    "                        \n",
    "                        # Guardar el DataFrame actualizado en el archivo chart\n",
    "                        df_chart.to_csv(ruta_archivo_chart, index=False)\n",
    "                        print(f\"Archivo actualizado: {ruta_archivo_chart}\")\n",
    "    else:\n",
    "        print(f\"El archivo {ruta_archivo_metric} está vacío y no se procesará.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta de entrada (metrics/charts)\n",
    "carpeta_charts = os.path.join('metrics', 'charts')\n",
    "\n",
    "# Lista de prefijos de archivos a procesar\n",
    "prefijos = ['ema50', 'ema100', 'ema200', 'price']\n",
    "\n",
    "# Ruta del archivo ema100-silhouette.csv\n",
    "archivo_ema100_silhouette = os.path.join(carpeta_charts, 'ema100-silhouette.csv')\n",
    "\n",
    "# Leer el archivo ema100-silhouette.csv para obtener la columna 'method'\n",
    "df_ema100_silhouette = pd.read_csv(archivo_ema100_silhouette)\n",
    "\n",
    "# Comprobar si la columna 'method' existe en el DataFrame\n",
    "if 'method' in df_ema100_silhouette.columns:\n",
    "    method_values = df_ema100_silhouette['method']\n",
    "else:\n",
    "    raise ValueError(\"La columna 'method' no existe en el archivo ema100-silhouette.csv\")\n",
    "\n",
    "# Lista de archivos en la carpeta charts\n",
    "archivos_charts = [archivo for archivo in os.listdir(carpeta_charts) if archivo.endswith('.csv')]\n",
    "\n",
    "# Función para verificar si el nombre del archivo empieza con alguno de los prefijos dados\n",
    "def empieza_con_prefijo(nombre_archivo, prefijos):\n",
    "    return any(nombre_archivo.startswith(prefijo) for prefijo in prefijos)\n",
    "\n",
    "# Procesar cada archivo CSV en charts\n",
    "for archivo_chart in archivos_charts:\n",
    "    if empieza_con_prefijo(archivo_chart, prefijos):\n",
    "        ruta_archivo_chart = os.path.join(carpeta_charts, archivo_chart)\n",
    "        \n",
    "        # Leer el archivo CSV\n",
    "        df_chart = pd.read_csv(ruta_archivo_chart)\n",
    "        \n",
    "        # Comprobar si la columna 'iteration' existe en el DataFrame\n",
    "        if 'iteration' in df_chart.columns:\n",
    "            # Sustituir la columna 'iteration' con los valores de la columna 'method'\n",
    "            df_chart['iteration'] = method_values\n",
    "            \n",
    "            # Guardar el DataFrame actualizado en el archivo chart\n",
    "            df_chart.to_csv(ruta_archivo_chart, index=False)\n",
    "            print(f\"Columna 'iteration' sustituida en: {ruta_archivo_chart}\")\n",
    "        else:\n",
    "            print(f\"La columna 'iteration' no existe en: {ruta_archivo_chart}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01589138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta de entrada (metrics/charts)\n",
    "carpeta_charts = os.path.join('metrics', 'charts')\n",
    "\n",
    "# Lista de archivos en la carpeta charts\n",
    "archivos_charts = [archivo for archivo in os.listdir(carpeta_charts) if archivo.endswith('.csv')]\n",
    "\n",
    "# Diccionario de condiciones y las columnas a eliminar\n",
    "condiciones_columnas = {\n",
    "    \"calinski\": [\"Silhouette\", \"Dunn\", \"Davies-Bouldin\", \"time\"],\n",
    "    \"davies\": [\"Silhouette\", \"Dunn\", \"Calinski-Harabasz\", \"time\"],\n",
    "    \"dunn\": [\"Silhouette\", \"Calinski-Harabasz\", \"Davies-Bouldin\", \"time\"],\n",
    "    \"time\": [\"Silhouette\", \"Dunn\", \"Davies-Bouldin\", \"Calinski-Harabasz\"],\n",
    "    \"silhouette\": [\"Calinski-Harabasz\", \"Dunn\", \"Davies-Bouldin\", \"time\"]\n",
    "}\n",
    "\n",
    "# Procesar cada archivo CSV en charts\n",
    "for archivo_chart in archivos_charts:\n",
    "    ruta_archivo_chart = os.path.join(carpeta_charts, archivo_chart)\n",
    "    \n",
    "    # Leer el archivo CSV\n",
    "    df_chart = pd.read_csv(ruta_archivo_chart)\n",
    "    \n",
    "    # Determinar las columnas a eliminar según el nombre del archivo\n",
    "    columnas_a_eliminar = []\n",
    "    for key, columnas in condiciones_columnas.items():\n",
    "        if key in archivo_chart:\n",
    "            columnas_a_eliminar.extend(columnas)\n",
    "    \n",
    "    # Eliminar las columnas especificadas si existen en el DataFrame\n",
    "    columnas_a_eliminar = list(set(columnas_a_eliminar))  # Eliminar duplicados\n",
    "    columnas_existentes_a_eliminar = [col for col in columnas_a_eliminar if col in df_chart.columns]\n",
    "    \n",
    "    if columnas_existentes_a_eliminar:\n",
    "        df_chart.drop(columns=columnas_existentes_a_eliminar, inplace=True)\n",
    "        # Guardar el DataFrame actualizado en el archivo chart\n",
    "        df_chart.to_csv(ruta_archivo_chart, index=False)\n",
    "        print(f\"Columnas {columnas_existentes_a_eliminar} eliminadas en: {ruta_archivo_chart}\")\n",
    "    else:\n",
    "        print(f\"No hay columnas para eliminar en: {ruta_archivo_chart}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta de entrada (metrics/charts)\n",
    "carpeta_charts = os.path.join('metrics', 'charts')\n",
    "\n",
    "# Lista de archivos en la carpeta charts\n",
    "archivos_charts = [archivo for archivo in os.listdir(carpeta_charts) if archivo.endswith('.csv')]\n",
    "\n",
    "# Procesar cada archivo CSV en charts\n",
    "for archivo_chart in archivos_charts:\n",
    "    ruta_archivo_chart = os.path.join(carpeta_charts, archivo_chart)\n",
    "    \n",
    "    # Leer el archivo CSV\n",
    "    df_chart = pd.read_csv(ruta_archivo_chart)\n",
    "    \n",
    "    # Verificar si la columna 'iteration' existe y renombrarla a 'method'\n",
    "    if 'iteration' in df_chart.columns:\n",
    "        df_chart.rename(columns={'iteration': 'method'}, inplace=True)\n",
    "        \n",
    "        # Guardar el DataFrame actualizado en el archivo chart\n",
    "        df_chart.to_csv(ruta_archivo_chart, index=False)\n",
    "        print(f\"Columna 'iteration' renombrada a 'method' en: {ruta_archivo_chart}\")\n",
    "    else:\n",
    "        print(f\"La columna 'iteration' no existe en: {ruta_archivo_chart}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2eb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de la carpeta de entrada (metrics/charts)\n",
    "carpeta_charts = os.path.join('metrics', 'charts')\n",
    "\n",
    "# Lista de archivos en la carpeta charts\n",
    "archivos_charts = [archivo for archivo in os.listdir(carpeta_charts) if archivo.endswith('.csv')]\n",
    "\n",
    "# Columnas a conservar\n",
    "columnas_a_conservar = ['clusters', 'iteration', 'time']\n",
    "\n",
    "# Procesar cada archivo CSV en charts\n",
    "for archivo_chart in archivos_charts:\n",
    "    if archivo_chart.endswith('time.csv'):\n",
    "        ruta_archivo_chart = os.path.join(carpeta_charts, archivo_chart)\n",
    "        \n",
    "        # Leer el archivo CSV\n",
    "        df_chart = pd.read_csv(ruta_archivo_chart)\n",
    "        \n",
    "        # Conservar solo las columnas especificadas\n",
    "        columnas_existentes_a_conservar = [col for col in columnas_a_conservar if col in df_chart.columns]\n",
    "        df_chart = df_chart[columnas_existentes_a_conservar]\n",
    "        \n",
    "        # Guardar el DataFrame actualizado en el archivo chart\n",
    "        df_chart.to_csv(ruta_archivo_chart, index=False)\n",
    "        print(f\"Archivo procesado y actualizado: {ruta_archivo_chart}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo sk-timeserieskmeans-silhouette.csv\n",
    "ruta_archivo_base = os.path.join('metrics', 'charts', 'sk-timeserieskmeans-silhouette.csv')\n",
    "\n",
    "# Leer el archivo base para obtener la columna 'field'\n",
    "df_base = pd.read_csv(ruta_archivo_base)\n",
    "\n",
    "# Verificar si la columna 'field' existe en el DataFrame base\n",
    "if 'field' in df_base.columns:\n",
    "    columna_field = df_base['field']\n",
    "else:\n",
    "    raise ValueError(\"La columna 'field' no existe en el archivo base.\")\n",
    "\n",
    "# Ruta de la carpeta de entrada (metrics/charts)\n",
    "carpeta_charts = os.path.join('metrics', 'charts')\n",
    "\n",
    "# Lista de archivos en la carpeta charts\n",
    "archivos_charts = [archivo for archivo in os.listdir(carpeta_charts) if archivo.endswith('.csv')]\n",
    "\n",
    "# Procesar cada archivo CSV en charts\n",
    "for archivo_chart in archivos_charts:\n",
    "    ruta_archivo_chart = os.path.join(carpeta_charts, archivo_chart)\n",
    "    \n",
    "    # Leer el archivo CSV\n",
    "    df_chart = pd.read_csv(ruta_archivo_chart)\n",
    "    \n",
    "    # Verificar si la columna 'iteration' existe en el DataFrame\n",
    "    if 'iteration' in df_chart.columns:\n",
    "        # Sustituir la columna 'iteration' con la columna 'field' del archivo base\n",
    "        df_chart['iteration'] = columna_field\n",
    "        \n",
    "        # Guardar el DataFrame actualizado en el archivo chart\n",
    "        df_chart.to_csv(ruta_archivo_chart, index=False)\n",
    "        print(f\"Columna 'iteration' sustituida por 'field' en: {ruta_archivo_chart}\")\n",
    "    else:\n",
    "        print(f\"La columna 'iteration' no existe en: {ruta_archivo_chart}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f07e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta de entrada (metrics)\n",
    "carpeta_metrics = 'metrics/charts-fields'\n",
    "\n",
    "# Lista para almacenar los archivos CSV encontrados\n",
    "archivos_csv = []\n",
    "\n",
    "# Recorrer los directorios y archivos dentro de la carpeta metrics\n",
    "for root, dirs, files in os.walk(carpeta_metrics):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            archivos_csv.append(os.path.join(root, file))\n",
    "\n",
    "# Procesar cada archivo CSV encontrado\n",
    "for archivo in archivos_csv:\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(archivo)\n",
    "    \n",
    "    # Verificar si la columna 'iteration' existe en el DataFrame\n",
    "    if 'iteration' in df.columns:\n",
    "        # Cambiar el nombre de la columna 'iteration' a 'field'\n",
    "        df.rename(columns={'iteration': 'field'}, inplace=True)\n",
    "        \n",
    "        # Guardar el DataFrame actualizado en el archivo CSV\n",
    "        df.to_csv(archivo, index=False)\n",
    "        print(f\"Columna 'iteration' renombrada a 'field' en: {archivo}\")\n",
    "    else:\n",
    "        print(f\"La columna 'iteration' no existe en: {archivo}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
